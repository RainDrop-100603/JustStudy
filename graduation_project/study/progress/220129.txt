=====summary=====

목표
    신호 전처리
        신호의 가공 형태
    	    MFCC
    	    Mel-spectrogram
    	    Spectrogram
        음악 신호를 자르는 단위 
        	Per frame (20~40ms)
            Per piano 박자
        가공된 신호의 사용 방법
            전체주파수 대역 
            타겟주파수 대역 (피아노가 사용하는 주파수 대역)
    딥러닝 모델
        음성, 음악과 같은 동적인 모델의 딥러닝 방법
        신경망 모델
            denseNet  
    딥러닝 출력
        회귀(regression)인가 분류(classification)인가?
            회귀(regression) 
            분류(classification)
    기타    
        음악 신호의 이해
            피아노 신호의 특성
            음악 신호의 특성
        구글 코랩 사용방법 -
        입력 음악파일 확보 

나의 생각, 음악 처리
    data를 특정한 간격으로 자르고, n개의 자른 조각을 한번에 학습 
        이때 다음 학습은 now+n이 아니라, now+n/2로 하는 등, 조각들이 겹치게 학습
        따라서 data를 어떤 간격으로 자르느냐, 조각을 몇개나 학습하냐, 얼마나 겹치게 하느냐가 다 중요할듯 
        STFT를 하면 조각을 겹치게 할 필요가 있나? 
            그래도 겹치게 하긴 해야할거 같기도 하고 
    하이퍼 파라미터를 정할 때, 직접만든 음원을 이용하여 분석하면 좋을듯 
        피아노, 기타, 베이스 등을 합친 음원을 실제로 분석하면 어떻게 나오는지 분석 
    초기 가중치 W를 정할 때, 우리가 만든 clean한 데이터를 사용하는 것도 ㄱㅊ을 듯?
        clean 데이터로 가중치를 미리 만들면, 유효한 값을 위주로 학습할 듯 하다. 
    데이터 가공 학습 
        1. 피아노, 기타 등의 여러 악기의 음 생성
            도, 레, 미 등 단음 생성
            도미솔 등 화음 생성 
            잡음 포함하여 생성 or 악기를 섞어서 생성  
        2. spectrogram, m-spectrogram, MFCC로 정리하여 특성 확인 
        3. 실제 음원을 잘라서 한번 비교도 해보기         

    
todo 
    실험 데이터 생성 
        피아노 base 데이터 생성
            모든 88음계의 단음 생성
            몇개의 화음 생성 
        다른 악기 base 데이터 생성
            모든 음계를 할 필요는 없고, 주요 음계 몇개만 
            피아노와 비교하기 위함임 
        피아노와 다른 악기를 섞은 데이터 생성 
            모든 음계를 할 필요는 없음
            섞였을때의 특성을 보기 위함
            주로 사용하는 악기랑 섞거나, 음역대가 비슷한 악기를 섞어본다. 
    실험 데이터 가공 
        spectrogram, m-spectrogram, MFCC등으로 정리하여 특성을 확인해보기 
        실제 음원을 잘라서 확인해 볼 수도 있다. 







======study======
mel-spectrogram은 spectrogram을 mel-scale로 변환하여 처리한 것이다. 주로 음성데이터를 처리할 때 많이 사용한다. 그런데 피아노는 음성데이터가 아닌데, mel-scale로 처리하는 것이 적절한지에 대한 의문이 존재한다. 따라서 spectrogram에 대한 조사도 추가하였다.
                한가지 고려하여야 할 점은, 피아노의 배음도 결국 인간이 듣기 좋은 형태로 형성되었다는 점이다. 따라서 mel-spectrogram을 이용하는 것이 적절할 수도 있다.
                기본적으로 spectrogram과 mel-spectrogram의 형태가 비슷하기 때문에, 같은 딥러닝에 두 데이터를 모두 활용 가능할 수도 있다.

3.1.1	회귀(regression) 
 회귀의 경우 MFCC와 같이, 입력과 같은 데이터 형식으로 출력하는 것을 말한다. 이때 정답 label은 유저가 임의로 생성한 것을 사용한다. 
 위와 같은 방식은, 출력 결과를 다시 정답 label과 비교하는 딥러닝을 수행해야 하는 단점이 있다. 즉 두 단계로 이어진 딥러닝을 수행해야 한다는 것이다. 그리고 인간의 오차(건반을 기계와 똑같이 칠 수는 없다)로 인해 오히려 부족한 결과를 낼 수도 있다.
3.1.2	분류(classification)
 출력의 형태는 피아노의 음, 정확하게 말하면 타이밍(타건 시점). 지속시간(음의 지속), 음의 높이가 출력될 것이다.
 타이밍의 경우, 입력을 잘라서 사용하기 때문에 특정하기가 쉽다.
 지속시간의 경우, 피아노 음의 대한 이해가 필요할 것이다.
 음의 높이의 경우도 피아노에 음에 대한 이해가 필요하다. 

피아노의 신호의 특성 
    https://kr.yamaha.com/ko/products/contents/musical_instrument_guide/piano/trivia/trivia007.html
    https://m.blog.naver.com/shinbygirl82/220211818842
    피아노의 음향은 매우 다양하게 나타날 수 있으며, 이것을 모두 정형화 시키거나 하는것으 어렵다고 본다
        -> 즉, feature를 뽑기보단 end-to-end에 가까운 학습방법을 사용하는것이 좋을 것 같다. 
        

4.4 멜로다인 

4.5	단순한 음원에서 복잡한 음원의 사용
 음악과 관련된 딥러닝은 자료가 부족하고, 우리의 시도가 어느정도의 난이도를 가지는지 측정하기 어렵다. 따라서 보컬이 없고 악기의 개수가 적은 음원부터 학습해보고, 성능이 괜찮다면 복잡한 음원에 대해서도 시도해 볼 수 있을 것이다.
 단순한 음원에서 복잡한 음원으로의 전환할 때, 딥러닝을 새로 시작할 수도 있지만, 기존의 학습된 가중치를 가지고 시도할 수도 있다. 보다 깨끗한 데이터에서 얻은 초기값을 통하여 더 나은 결과를 도출할 수 있다고 본다. (즉 ㄱ

4.6	Spleeter
https://github.com/deezer/spleeter/blob/master/README.md
음원을 자동으로 분류해주며(텐서플로우 이용), 피아노도 포함해서 분류가 가능하다. 해당 라이브러리를 뜯어봐서 도움을 얻거나, 피아노를 분류시킨 데이터를 이용해서 학습시키는 것도 가능할 듯 하다. 
4.7	미분류
자동 채보라 검색하면 검색 결과 이것저것 나온다. 
http://www.aitimes.com/news/articleView.html?idxno=140852 
https://www.mapianist.com/community/35109




T아카데미의 음성 딥러닝 강의, DSP부분과 딥러닝 부분으로 나뉘며, DSP부분은 spectrogram등 정보가 있다.


토크ON74차. 디지털신호처리 이해 | T아카데미
https://youtube.com/playlist?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN

1강: [토크ON세미나] 디지털신호처리 이해 1강 - 디지털신호처리(DSP) 기초 I - Sampling, Quantization | T아카데미 
    https://youtu.be/RxbkEjV7c0o

    음성처리 동향: feature 추출 에서 end-to-end로 가는중 

    사용한 tool: torch, torchaudio

    음성을 다룰 때 입력의 형태: spectrogram
    spectrogram

    컴퓨터가 음성을 처리하는 방식
        Sampling rate: 초당 몇번 sampling 할 것인가
            Nyquist theorem: 최대 sr의 두배로 sampling하면, 데이터를 잃지 않고 추출할 수 있다
                (물론 실제로는 Quantization때문에 데이터를 약간 잃는다.)
                    frequency관련해서는 안 잃던가? DSP복습 하면 괜찮을듯 
            음성별로 sr을 정해도 되지만, speech, music등 영역별로 정형화된 sr이 존재한다.
                music은 22.05kHz혹은 44.1kHz, speech는 그거보다 낮다.
        Quantization: 음은 연속적이지만 sampling가능한 단계는 연속적이지 못하다.
            예를들어 "도"와 "레" 사이에는 무수히 많은 음이 있지만, 모두를 표현할 수 없다.
            적절한 수준으로 양자화하여 사용한다.
            B bit 사용시, -2^(B-1)~2^(B-1)-1 
        Normalization: Quantization 값을 -1 ~ 1 사이의 값으로 처리

    sr에 따른 성능 차이가 존재하는가?
        16k와 8k로 했을 때 비슷했었다.
        근데 학습 시간도 비슷했었다.
        
    소리의 표현 방식
    time-domain representation: 시간에 따른 amplitude(음압)의 변화
    time-frequency representation: 시간에 따른 frequency의 변화 

    보통은 audio input을 받으면 spectrogram으로 바꿔버리기 때문에 Quantization을 안 한다.
        근데 어차피 우리는 이미 Quantization된 음악파일을 이용한다. 

    소리의 물리량
        amplitude: 진폭
        frequency: 주파수
        phase: 위상 
    음향?
        소리의 특징을 구분하는 방법? 
            물리는 물리적인 특성, 심리는 우리가 느끼는 특성을 말하는 것 같기도 
        물리 음향
            intensity: 진폭의 세기
            frequency: 떨림의 빠르기
            tone-color: 파동의 모양
        심리 음향
            loudness: 소리 크기
            pitch: 음정, 소리의 높낮이/진동수
            timbre: 음색, 소리 감각
        
    딥러닝에는 보통 amplitude, frequency를 이용한다. 
        음악을 다룰때는 또 모르겠다.
        frequency는 음의 높낮이를 결정 

    complex wave: 우리가 사용하는 대부분의 소리들은 정현파가 합쳐진 복합파다.
        정현파: 일종의 복소 주기함수, 피아노로 한 건반을 누르면 그것이 정현파다.
        ** 특정 주파수 영역만 사용하는것은, complex wave를 다룰때는 부적합 할 수도 있을듯 
            그런데 mel- spectrogram으로 보면 분리되서 나오는 듯? 

    1장에서 음을 직접 생성하는 법이 나온다. 물론 복잡한건 아니고 특정 주파수의 간단한 음인데, 추후 복잡하게 만들 수도 있으려나? 

2강: [토크ON세미나] 디지털신호처리 이해 2강 - 디지털신호처리(DSP) 기초 II - STFT, Spectrogram, Mel-Spectrogram | T아카데미
    https://youtu.be/FjYNM3YGFB4

    frequency에 대한 설명 

    sinusoidal wave(정현파)에 대한 설명 

    파형을 time-domain representation으로 시각화 시키는 방법

    normalize: 딥러닝시 성능 향상
    mu-law encoding: mel-spectrogram을 사용하므로 굳이 안해도 됨 
        mu-law encoding은 amplitude에 대한 것이고 mel은 freq에 대한 것
        실제 실험해 보든가?

    frequency를 얻는 법
        fourier transform: complex wave를 sinusoidal wave의 합으로 분해 
            복소 지수함수로 분해,즉 실수부와 허수부를 가지는 복소수가 얻어짐
            spectrum magnitude: 복소수의 절댓값, 주파수의강도
            phase spectrum: 복소수의 phase, 주파수의 위상 
        실수부가 freq, 허수부가 phase? 이건 뭔 소린지

    지수함수 or 주기함수? 
        오일러 공식: e^(iθ) = cosθ+isinθ
        즉, 표현 방식의 차이 

    DFT: discrete한 실제 digital data를 FT하는 방법 

    사람들이 실제로 소리를 들을 때는 한번에 긴 파장을 동시에 처리하는게 아니라, 짧은 신호를 연속해서 듣는것과 같다.
        그래서 나온게 STFT 

    STFT(Short Time fourier transform)  
        FT는 시간에 흐름에 따라 신호의 주파수가 변했을 때, 어느 시간대에 주파수가 변하는지 모르게 된다.
            헷갈리면, 그냥 사람이 소리를 듣는건 3분길이를 한번에 듣는게 아니라, 0.5초씩 듣는다고 생각
        사람은 0.5초정도 주기다? 
        아무튼 사람의 인식주기랑 비슷하게 잘라서 FT를 하면, 사람이 듣는 것과 비슷할 것이다. 
        hop size: 겹치게 하는 정도 
            0~1초, 1~2초를 하는 것 보단, 0~1초, 0.5~1.5초, 1~2초로 하는것, 즉 겹치게 하는것이 더 풍부한 정보를 얻을 수 있다.

    STFT는 라이브러리에서 구현된걸 쓰자, 직접 구현은 까다롭 

    audio deep_learning을 할 때는 허수부를 날린다고 함
        소리를 인지하는데 큰 info를 담고있다곤 생각되진 않는다고 함 
        허수부->phase
        날릴때 보통 절댓값의 제곱을 취해준다. 
            즉 허수부를 아예반영 안하는 건 아닌듯  

    STFT를 하면 m-spectrogram에서 본것과 같은, 그런 모양의 spectrogram을 얻을 수 있다. 

    STFT에서의 window size: STFT를 하면 데이터를 자르기 때문에 연속성이 깨지는 경우가 발생하는데, 이를 방지하기 위함
        window function을 곱해주면 양 끝부분이 0에 수렴한다.
        (파동의 모양에서 양 끝부분의 반영분이 낮아진다고 봐도 될듯 )

    N_FFT는 얼마로? -> 연산효율을 위해선 보통 1024
    window length와 N_FFT는 일치시키는게 좋다.
    hop_size: 1/2, 1/4등 알아서 
    음성은 1024(N_FFT), 512(hop_size),1024(win_length)의 경우 많은 경우에 쓸만하다.
    음악일 경우에는 또 다를 것이다.
        -> 피아노니까 피아노박자로 맞추는 것도 좋은 선택인듯 

    power_to_db: 출력단을 db단위로 표현 

    mel-단위 
        소리는 저주파 단위가 더 구분을 잘하고 풍부하다.
        설명이 상세히 나와있으니 추후 정리하면 좋을듯
        basilar membrane은 각각 위치 영역이 특정 freq에 맵핑되어 있는데 ~ 
            이론 정리할 때 쓰면 될듯 
        mel 단위를 써야하나?
            악기의 음 또한 아마 사람이 듣는걸 고려해서 설계되었을 것이다.
            따라서 mel-spectrogram을 사용하는것도 괜찮을 듯 하다. 
            음성,음악모두 mel-scale을 쓰면 더 성능이 좋다고 함 
        특정 주파수 영역의 가중치를 높이고, 다른 영역을 낮추는 것임 
           
    ft하지 않고 그냥 주파수를 넣어서(wave form, amplitude만 보여주는 우리가 아는 그것) end-to-end로 학습하려는 시도도 있음 
        아직까지는 spectrogram이 많지만, wave form단위로 내려가려는 시도롤 하는 듯 

    n_mel (mel bin) 
        음성은 40, 음악은 92나 128 
    악기별로 spectrogram을 뽑아서, 어느 영역까지 필요한지 보고 mel bin을 정하면 ㄱㅊ을듯 
        이 부분은 영상 다시 보면서 복습하거나 하자 

    phase를 항상 날리는건 아님 
        magenta참고 

3강:[토크ON세미나] 디지털신호처리 이해 3강 - 디지털신호처리(DSP) 기초 III - MFCC, Auditory Filterbank | T아카데미
    https://youtu.be/kiTHOCmWPsg

    db를 amplitude ratio기준으로 하나, power ratio 기준으로 하나?
        wave-form기준을 그대로 쓰면된다. 
        wave form이면 보통 amplitude인듯 
        보통 power to db는 안쓰고 amplitude를 쓴다. 

    MFCC 
        얻는법
            mel-spectrum에 log 적용 
            mel-log-spectrum list 전체에 DCT(discrete cosine transform)
            얻어진 coefficients에서 앞에서부터 N개만 남기고 버린다. 
        feature engineering시절에 많이 쓰임
            요즘에는 spectrogram level로 넣는 경우가 많다. 
            가공을 할수록 차원이 줄어듬 -> 정보가 줄어듬 
            powerful한 feature engineering이기는 하다.   

    waveform->spectrogram->m-spectrogram->MFCC 로 갈 수록 정보가 줄어든, feature를 뽑아주는 느낌인듯 

4강: [토크ON세미나] 디지털신호처리 이해 4강 - 디지털신호처리(DSP) 실습 I - Data augmentation | T아카데미
    https://youtu.be/VzR0hBVZvRA

    Data augmentation 
        1. wave form level에서 noise를 섞는등의 가공
        2. spectrogram level에서 마스킹을 주는 방법 
        3. data split - task 마다 다르다. 

    1번 2번에 대해서 실습 

    wave-form 가공  
        pitch-shift: 음의 옥타브를 바꾸어서 새로운 데이터를 만듬 
            librosa.effects.pitch_shift를 통해 구현 
        value_augmentation: amplitude를 랜덤하게 키운다.
            음성에서만 이용되는 방법일듯? 
            별로 잘 안된다고 함
        noise: 노이즈도 잘 되지는 않음 
            노이즈는 의미있는 수준으로 해야하기 때문에, 귀로 들어보며 추가하는것도 ㄱㅊ 
        hpss(harmonic percussive): 음악쪽에서 많이 씀
            멜로디라인, 음악라인을 부분해서 씀
            노래하는 음악이면 꽤나 괜찮다. 
        shift: 순서 도치, 내가 말을 한다-> 말을 내가 한다와 같은 것 
            잘 안됨
        stretch: 늘리거나 줄여줌, 빨리감기라 생각하면 편함 
        change_speed: stretch랑 비슷 
        음성의 경우 pitch-shift랑 stretch가 좋았다고 함(하나씩 할때는)
            실제 할때는 다 적용시켜 버려서 모르겠다고 함

    spectrogram 가공 
        masking: time이랑 frequency를 전부 날려버림, 0처리(특정 부분)


    음악은 effector나 리벌브? 를 넣는다거나 (9:00)
        음악논문 같은 경우 augmentation을 잘 안적어 놓는다고 한다. 

    augmentation은 train set에서만 사용 

    이미지를 반반씩 섞는 방법론도 있음
    mixup, medifold mixup(cnn레일를 몇개 거친 것을 mixup하는 경우도 있음)


5강: [토크ON세미나] 디지털신호처리 이해 5강 - 디지털신호처리(DSP) 실습 II - DataLoader | T아카데미
    https://youtu.be/oFdjv1CJRWo
    
    input을 처리하는 방법들 
    1. spectrogram 등 "전처리된 input"을 저장하여 불러오는 방법
    2. "waveform"을 input으로 넣고, batch를 할 때마다 전처리를 하도록 처리 
        하이퍼 파라미터를 정하기 위해 실험을 하는 등, 여러 변수를 바꾸면서 하고싶을 때

    spectrogram은 normalize를 하는것이 좋다 

    masking을 one-hot(일괄적으로)으로 할 수도 있고, 특정 조건에 따라 만들수도 있다(특정 주파수를 넘어가면 선형적으로 0을 향해가는 등)

    측정방법 MOS? 음성 합성 분야 말하는듯 
        
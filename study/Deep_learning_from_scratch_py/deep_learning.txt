---책에서의 표현---
뉴런 = 노드 

---ch2 perceptron---
perceptron: 입력을 받으면 가중치와 편향에 따라 출력을 반환하는 알고리즘 
단층 perceptron: 
    AND,NAND,OR을 구현할 수 있다. 
    선형 영역을 표현한다.(좌표평면으로 나타내면 직선을 기준으로 값이 나뉜다.)
    y = (w1*x1+wx*x2>θ), x = 입력, w = 가중치,  θ = 임계값
    y = (w1*x1+w2x2+b>0), b = -θ = 편향 (b는 입계값의 다른 표현이다.)
다층 perceptron: 
    XOR을 구현할 수 있다.
    비선형 영역을 표현한다.(좌표평면으로 나타내면 곡선을 기준으로 값이 나뉜다.)
    x XOR y = (x OR y) AND (x NAND y) 
이론상으론 다층퍼셉트론으로 컴퓨터를 구현할 수 있다. 즉 복잡한 함수들도 모두 구현할 수 있다. 

---ch3 신경망---
활성화함수(activation func): 입력의 처리결과에 따른 출력을 나타내는 함수 
    활성화함수 h(a), a = b + w1x1 + w2x2 
        step func: h(a) = 1(a>0), 0(a<=0)
        sigmoid func: h(a) = 1/(1+exp(-a))
        ReLU func: h(a) = a(a>0), 0(a<=0)
            Rectified Linear Unit, 최근에 주로 이용하는 활성화 함수다.

    신경망에서는 비선형 함수를 이용해야한다.(위의 활성화 함수는 모두 비선형)
        선형함수는 아무리 층을 깊게 해도, 은닉층이 없는 네트워크로도 같은 기능을 할 수 있다.
            ex): h(a) = c*a -> h(h(h(a))) = c^3 * a (선형함수) 

    일반적으로 단순 퍼셉트론은 단층 네트워크에서 활성화 함수로 계단 함수를 이용한 것을, 
        다층 퍼셉트론은 신경망(다층 네트워크로 구성되고 계단함수보다 매끄러운 활성화함수 이용)을 나타난다.

    은닉층의 활성화 함수와 출력층의 활성화 함수가 다를 수 있다.
        출력층에는 목적에 따라 다양한 활성화 함수를 이용한다. 추후 다시 설명 
            항등함수: 회귀(입력데이터에서 연속적인 수치를 예측하는 문제)
            소프트맥스함수: 분류(데이터가 어느 클래스에 속하는지를 분류하는 문제) 

신경망의 순전파(forward propagation) 계산 
    신경만의 각 층의 계산은, 행렬 곱을 이용하여 간단하계 계산할 수 있다. 
    행렬의 곱은 np.dot(arrA,arrB)로 구현한다. 

출력층:
    추가적인 활성화함수를 사용한다. 회귀에서는 항등함수를, 분류에서는 소프트맥스함수를 사용하는것이 일반적 
        항등함수
            y = x 
        소프트맥스함수 
            y = np.exp(x) / np.sum(np.exp(x))
                -> 지수함수계산으로 인한 오버플로우등의 오류가 발생할 수 있다. 
            개선: 
                y = c*np.exp(x) / c*np.sum(np.exp(x))
                    = np.exp(x+log(c)) / np.sum(np.exp(x+log(c)))
                    = np.exp(x+C) / np.sum(np.exp(x+C))
                즉, 어떠한 값을 더하거나 빼도 결과는 같다는 것이다.
                일반적으로, max(x)값을 빼준다. 즉 일반적으로 C = -max(x) 이다. 
            특징:
                출력값이 0 ~ 1 사이의 실수다.
                출력값의 합이 1이다. 
                -> 소프트맥수 함수의 출력을 "확률" 이라고 해석할 수 있다. 
            신경망으로 "분류"할때는 생략하기도 한다.  
                학습단계에서는 출력함수를 이용하고(연산을 해야하니), 
                    추론(실사용)단계에서는 출력함수를 생략하는것이 일반적 (어차피 제일 높은거 하나로 분류하면 되니까)
    분류에서의 출력층의 뉴런 수 = 분류하고 싶은 클래스의 개수 로 설정하는것이 일반적 

손글씨 숫자인식 
    이미지를 픽셀로 쪼개고, 각 픽셀은 0~255사이의 값을 취한다. 

전처리(pre-processing)
    정규화: 전처리를 통해 식별능력을 개선하고 학습속도를 높일 수 있다. 
        ex) 데이터 전체 평균과 표준편차를 이용하여 0을 중심으로 분포하도록 이동하거나, 데이터 확산범위를 제한한다.
    백색화: 전체 데이터를 균일하게 분포시키는 방법, 추후 추가바람 

배치(batch)
    하나로 묶은 입력 데이터 
        MNIST에서의 행렬 연산은 아래와 같다.
            X(1x784)·W1(784x50)·W2(50x100)·W3(100x10) = Y(1x10)
        입력을 묶어서 넣을 수 있으며(100x784), 이렇게 하나로 묶은 입력 데이터를 batch라 한다. 
            X(100x784)·W1(784x50)·W2(50x100)·W3(100x10) = Y(100x10)
    속도의 이득     
        수치 계산 라이브러리 대부분이 고도로 최적화되어있어, 큰 배열 연산을 효율적으로 처리 가능하다.
        I/O 횟수를 줄여 실제 연산에 투자하는 비율을 늘려 속도를 증가시키고, BUS에 주는 부하도 감소시켜 병목도 방지한다.

---ch4 신경망 학습---
    학습: 가중치, 편향등의 매개변수를 컴퓨터 스스로가 수정 
        퍼셉트론(단일) vs 신경망(다중 퍼셉트론)
            퍼셉트론또한 학습이 가능하지만, 선형분리만 가능하기 때문에 선형분리문제만 풀 수 있다. 
    머신러닝 vs 딥러닝
        머신러닝은 "특징"은 사람이 추출하고 학습은 기계가 한다. 
        딥러닝은 특징추출과 학습까지 모두 기계가 한다.(하지만 추출된 특징을 사람이 판별하긴 어렵다.)
            딥러닝을 종단간 기계학습(end-to-end machine learning)이라고도 한다.
            특징을 추출할 필요가 없기 때문에 비슷한 문제는 같은 맥락에서 풀 수 있다. (ex: 이미지 분별은 모두 같은맥락)
            
    오버피팅(overfitting)
        특정한 학습데이터에 너무 최적화 되어서, 다른 문제는 풀지 못하는 경우 
    
    손실함수(loss function)
        최적의 매개변수를 탐색하는 학습의 지표를 딥러닝에선 손실함수라고 한다.
        손실함수는 신경망 성능의 나쁨을 나타내는 지표, 즉 얼마나 데이터를 잘 처리하지 못하느냐의 지표다.
        여러 손실함수가 있지만, 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용한다. 

        정답 레이블의 방법  
            숫자 레이블: 숫자 딥러닝을 예시로 들면, 0~9 중 하나인 값(스칼라)
            원-핫 인코딩: 정답은 1이고 나머지는 모두 0인 배열
                숫자 딥러닝을 예시로 들면, 배열의 크기는 항상 10이며 정답만 1이고 나머지는 0이다. 
            의문: 정답은 확실하므로 1인것이 맞긴하지만, 1이 아니고 [0.3, 0.7] 과 같이 주어지는 경우가 있는가? 

        오차제곱합(SSE, sum of square for error)
            E = 0.5 * sum((yk-tk)^2), y = 신경망의 출력, t = 정답, k = 데이터의 차원 수 
                주의: k값이 idx가 아니라 데이터의 차원 수 임을 유의하라.
                    예를들어 숫자인식은 0~9 까지의 확률이 존재하므로, k는 0~9까지 있다. 그 확률들에 대해 오차제곱합을 구하는 것이다. 
                오차제곱합이 낮을수록 정답에 근접한다.
                정답 형태는 배열이며 y와 t 의 배열크기는 같다. 원-핫 인코딩과 같은 정답이 사용될 수 있다.

        교차 엔트로피 오차(CEE, cross entropy error)
            E = -sum(tk*log(yk+delta)), y = 신경망의 출력, t = 정답, k = 데이터의 차원 수, delta = 아주작은수(연산오류방지) 
                정답 형태는 배열이며 y와 t 의 배열크기는 같다. 원-핫 인코딩과 같은 정답이 사용될 수 있다.

        훈련 데이터 모두에 대한 손실함수의 평균을 구하는 방법 
            avgE = sum(E) / N
            이 경우, 훈련데이터가 많다면 시간이 지나치게 오래걸릴 수 있다.

        mini-batch 학습: 훈련 데이터 일부에 대한 손실함수의 평균을 구해 근사치로 이용하는 방법 
            선택한 일부 데이터는 mini-batch, 이를 이용한 학습이 mini-batch 학습이다. 

        손실함수를 사용하는 이유: 손실함수가 0에 가까울 수록 좋다 -> 미분을 통해 매개변수의 값을 갱신하기 쉽다.
            추가바람? 

        신경망의 학습에서 정확도 대신 손실함수를 이용하는 이유 
            손실함수를 이용한다면, 미분값이 0이되는 지점(minimum)을 찾으며 매개변수를 갱신할 수 있다.
                -> 손실함수는 연속적이므로 미분값을 통해 minimum을 향해 가면 된다. 
            정확도를 이용한다면, 매개변수의 미분이 대부분의 장소에서 0이기 때문에, 매개변수를 갱신할 수 없다. 
                -> 정확도는 연속적인 값이 아니라 불연속적인 값이다. 30/100, 55/100과 같이 제한적인 불연속적인 값을 갖는다.
                -> 이를 함수로 표현하면 계단함수가 되는데, 계단함수에서는 미분을 통한 매개변수 갱신이 불가능하다. 

    매개변수 갱신의 방법
        경사법
            기울기(미분값)을 기준으로 매개변수를 갱신할 방향을 정한다. 
        
        파이썬(프로그래밍)으로 미분값을 구하는 방법
            미분의 정의: f'(x) = lim (f(x+h)-f(x)) / h 
                문제1: h값을 너무 작게하면, 부동소수점 연산오류가 생길 가능성이 크다. 
                문제2: h값이 충분히 작지 못하면, 실제 미분값과 계산값의 오류가 커진다. 
                        h대신 -h를 대입해버리면, 정의상으로는 문제가 없지만, 계산상으로는 오차가 크다. 
            중앙(중심) 차분 방법:
                f'(x) = (f(x+h)-f(x-h)) / 2h
                
        편미분: 변수가 여러개일때의 미분.

    # 4.4하다 말았다.
            

===================이 아래로는 구현등을 하지 않고 읽기만 하고 정리=========================

4.4.1 경사법
    하이퍼 바라미터: 학습률과 같이, 인간이 설정해주어야하는 매개변수. 시험을 통해 임의로 결정한다.
        4.5의 가중치 매개변수의 초기화도 이에 포함되는듯. 초깃값이 결과에 유의미한 영향을 준다.  
4.4.2 신경망 학습에서의 기울기 
    신경망에서의 기울기는 가중치 매개변수에 대한 손실함수의 기울기 
        기울기 = dL/dW, W는 가중치, L은 손실함수 
        의미상으로, dL/dW의 각 원소는, 각 가중치가 변화했을때의 손실함수의 변화다.     
4.5.1 
    오차 역전파법 
        수치미분을 수행했을때와 비슷한 결과를 훨씬 빠르게 구할 수 있다. 
4.5.3 실험 데이터로 평가하기
    오버피팅을 막는 방법
        가중치 감소
        드롭아웃
        조기종료: 1에폭(전체학습크기/배치크기) 마다 훈련데이터와 시험데이터의 정확도 추이를 계산 
            만약 시험데이터의 정확도가 떨어지기 시작한다면, 그 이전 지점까지만 학습하도록 구현 

총평: 학습에 세부 구현방법에 대해서는 아직 정확히 알지 못한다. 그러나 학습의 총체적인 방법에 대해서는 이해했다. 
    1. 학습 데이터를 잘개 쪼개어 값을 label한다. 흑백사진의 경우 픽셀로 쪼개어 명도로 label이 가능하다. 
        **주의**: 음악이나 음원같이 동적인 데이터의 경우 달라질 수 있음을 유의하자. 
    2. 은닉층의 형태와 층의 개수를 설정, 가중치 매개변수W의 초기화, 손실함수 L과 기울기 dL/dW를 구하는 방법등 기초를 구현 
    3. 학습을 통해 W를 갱신한다.
        for repeat_count 
            W -= learning_rate * dL/dW 
        # repeat_count 와 learning_rate 는 사람이 설정하는 것이다. 
    3.1 오버피팅을 막기위한 기법을 적용하여 학습하도록 한다. 
    4. 학습완료 

    유의사항: 정적인 데이터, 이미지의 경우의 개념이다. 음원이라 가정할경우 고려할 사항은
        음원을 잘게 잘라서 써야할 것인가? 자른다면 어떻게 자를 것인가.
        악보에서의 음이 눌려진 부분만 정답으로 처리할 것인가? 아니면 음의 잔향까지도 고려할것인가 
            음이 눌려진 부분만 처리한다면, 어쩔수 없이 오차가 나는 부분은 어떡할 것인가? -> 어차피 확률임을 고려하면 문제되지 않을지도
            오히려 음의 잔향까지 고려한다면, 잔향이 언제까지 남을지 재단하는것이 더 어렵다. 
            악보대로의 타이밍으로만 하는것이 좋을 듯 하다. 
        음원의 특징을 이해햐아한다.  
            사진은 명도, 채도로 label할 수 있다. 음악은 어떻게 label할 것인가?
                furiel 변환등의 변환이 필요할 것인가, 아니면 그냥 주파수를 사용하나? 
            음원은 하나의 주파수로 이루어져있나? 여러 음이 섞이면 주파수 간섭이 일어나지 않는가?
            다중채널인 음악도 있을텐데, 그 경우는 어떻게 할 것인가? 
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7064170db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nn.Module\n",
    "    * nn.Module을 상속하여, 사용자 정의 model class를 만들 수 있다.\n",
    "    * module에 함수 계산만 정의해두면, 계산값과 gradient값을 반환한다.\n",
    "    * nn.Linear, nn.Sigmoid 등의 함수는 미리 정의된 nn.Module이라 생각하면 된다.\n",
    "        * 이런 미리 정의된 함수는 gradient를 반환하기에 연속으로 사용 불가능 하지만(logistic_regression함수 참고), nn.Module을 상속한 class에선 연속으로 사용 가능하다.\n",
    "\n",
    "* torch 함수\n",
    "    * torch.tensor.scatter_\n",
    "    * torch.squeeze(unsqueeze) \n",
    "        * squeeze: size가 1인 dimension을 제거, ex) A1BC1D -> ABCD\n",
    "        * unsqueeze(x,k): x에서 k번째 dimension에 size가 1인 dimension 추가\n",
    "            * ex) unsqueeze(x,1): ABC -> A1BC, unsqueeze(x,0): ABC -> 1ABC        \n",
    "    * F.cross_entropy(linear regression, y_arr)\n",
    "        * softmax classification은 linear regression-> softmax -> cross_entropy의 연산을 해야하며, label도 one_hot으로 바꿔주어야 한다.\n",
    "        * cross_entropy함수는 복잡한 softmax연산과 one_hot변환조차 생략할 수 있게 해준다.\n",
    "\n",
    "* 함수 설명\n",
    "    * logistic regression\n",
    "        * prediction = nn.Sigmoid(model(x_train)) 은 Linear model에서 2개의 값을 반환해서(결과, gradient) Sigmoid와 이어서 사용할 수 없다.\n",
    "    * logistic regression2 \n",
    "        * nn.Module을 상속한 class에서는 이유는 모르겠는데?(아마 super.__init__덕분에) Linear model과 Sigmoid를 이어서 사용할 수 있다. \n",
    "    * softmax classification\n",
    "        * cross_entropy를 이용하여 softmax연산과 one_hot변경을 생략하여 함수가 엄청 간단해졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "    x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "    y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "    x_train = torch.FloatTensor(x_data)\n",
    "    y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "    # W = torch.zeros((2,1),requires_grad=True)\n",
    "    # b = torch.zeros(1,requires_grad=True)\n",
    "    # optimizer = optim.SGD([W,b],lr=1)\n",
    "    model = nn.Linear(2, 1)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "    epochs = 1000\n",
    "    for epoch in range(1 + epochs):\n",
    "        # prediction = 1/(1+torch.exp(-(x_train.matmul(W)+b)))\n",
    "        # cost = -(y_train*torch.log(prediction)+(1-y_train)*torch.log(1-prediction)).mean()\n",
    "        print(model(x_train))\n",
    "        prediction = nn.Sigmoid(model(x_train))\n",
    "        cost = F.binary_cross_entropy(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            prediction = prediction >= torch.FloatTensor([0.5])\n",
    "            print(\n",
    "                \"Epoch: {:4d}/{}, Prediction: {} Cost: {:.6f}\".format(\n",
    "                    epoch, epochs, prediction.squeeze().detach(), cost.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # print(W,\"\\n\",b)\n",
    "    print(list(model.parameters())[0], \"\\n\", list(model.parameters)[1])\n",
    "\n",
    "\n",
    "logistic_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # nn.Module의 __init__ 을 실행\n",
    "        # self.linear = nn.Linear(2,1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.sigmoid(self.linear(x))\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def logistic_regression2():\n",
    "    x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "    y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "    x_train = torch.FloatTensor(x_data)\n",
    "    y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "    model = BinaryClassifier()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "    epochs = 1000\n",
    "    for epoch in range(1 + epochs):\n",
    "        prediction = model(x_train)\n",
    "        cost = F.binary_cross_entropy(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            prediction = prediction >= torch.FloatTensor([0.5])\n",
    "            correct_prediction = prediction.float() == y_train\n",
    "            accuracy = correct_prediction.sum().item() / len(correct_prediction) * 100\n",
    "            print(\n",
    "                \"Epoch: {:4d}/{}, Accuracy: {}% Cost: {:.6f}\".format(\n",
    "                    epoch, epochs, accuracy, cost.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # print(W,\"\\n\",b)\n",
    "    print(list(model.parameters()))\n",
    "\n",
    "\n",
    "logistic_regression2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/1000, Cost: 2.823855\n",
      "Epoch:  100/1000, Cost: 0.712041\n",
      "Epoch:  200/1000, Cost: 0.623341\n",
      "Epoch:  300/1000, Cost: 0.564995\n",
      "Epoch:  400/1000, Cost: 0.514439\n",
      "Epoch:  500/1000, Cost: 0.466889\n",
      "Epoch:  600/1000, Cost: 0.420623\n",
      "Epoch:  700/1000, Cost: 0.374839\n",
      "Epoch:  800/1000, Cost: 0.329233\n",
      "Epoch:  900/1000, Cost: 0.284424\n",
      "Epoch: 1000/1000, Cost: 0.246721\n"
     ]
    }
   ],
   "source": [
    "# class SoftmaxClassifier(nn.Module):\n",
    "#     # nn.Linear만 있을거면 없어도 된다.\n",
    "#     def __init__(self):\n",
    "#         self.super().__init__()\n",
    "#         self.linear = nn.Linear(4, 3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.linear(x)\n",
    "\n",
    "\n",
    "def softmax_classification():\n",
    "    x_data = [\n",
    "        [1, 2, 1, 1],\n",
    "        [2, 1, 3, 2],\n",
    "        [3, 1, 3, 4],\n",
    "        [4, 1, 5, 5],\n",
    "        [1, 7, 5, 5],\n",
    "        [1, 2, 5, 6],\n",
    "        [1, 6, 6, 6],\n",
    "        [1, 7, 7, 7],\n",
    "    ]\n",
    "    y_data = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "    x_train = torch.FloatTensor(x_data)\n",
    "    y_train = torch.LongTensor(y_data)\n",
    "\n",
    "    # F.cross_entropy를 사용하면 필요없다.\n",
    "    # y_onehot = torch.zeros(8, 3)\n",
    "    # y_onehot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "\n",
    "    # W = torch.zeros((4, 3), requires_grad=True)\n",
    "    # b = torch.zeros(1, requires_grad=True)\n",
    "    # optimizer = optim.SGD([W, b], lr=0.1)\n",
    "    model = nn.Linear(4, 3)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs + 1):\n",
    "        # softmax와 cross entropy를 정석대로 구현\n",
    "        # prediction = F.softmax(x_train.matmul(W)+b, dim=1) # dim=1: dim=1 내부에서 softmax, not 전체\n",
    "        # cost = (y_onehot* -torch.log(prediction)).sum(dim=1).mean() # dim=1: dim=1 내부에서 sum, not 전체\n",
    "        # cross_entropy 함수를 이용해 one_hot label도 안만들고, softmax도 직접구현 안함\n",
    "        # z = x_train.matmul(W) + b\n",
    "        # cost = F.cross_entropy(z, y_train)\n",
    "        # class를 이용해 더 간소화\n",
    "        prediction = model(x_train)\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: {:4d}/{}, Cost: {:.6f}\".format(epoch, epochs, cost.item()))\n",
    "\n",
    "\n",
    "softmax_classification()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('deep_learning-2nzCan8t')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47a76cc6cc717bae4e839f60eebec3290c838836c41780b1b6f292f9d88b61ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
